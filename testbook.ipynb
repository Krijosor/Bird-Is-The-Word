{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a667ce85",
   "metadata": {},
   "source": [
    "### Test of data instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21443577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchmetrics import FBetaScore, Accuracy\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from src import BWDatasets\n",
    "from paddleocr import PaddleOCR\n",
    "import paddle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b9f6a",
   "metadata": {},
   "source": [
    "### Ensure Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f722ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 115\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "np_generator = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79d1e5",
   "metadata": {},
   "source": [
    "### Activate CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d03023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda availability is: True\n",
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# Make sure the notebook is deterministic if training on gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f'Cuda availability is: {torch.cuda.is_available()}')  # Returns True if a GPU is available\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cb823",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Invalid file path. Path dataset\\datasets\\lyngoy\\images\\blub does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m max_n = \u001b[32m100\u001b[39m\n\u001b[32m     19\u001b[39m transform = T.Compose([\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# T.Resize((64,32)),\u001b[39;00m\n\u001b[32m     21\u001b[39m     T.Resize((\u001b[32m224\u001b[39m,\u001b[32m112\u001b[39m)),\n\u001b[32m     22\u001b[39m     T.Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: F.rotate(x, \u001b[32m270\u001b[39m, expand=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     23\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m exp_dataset = BWDatasets.TrainDataSet(img_path=image_path, labels_path=label_path, bb_path=bb_path, transform=transform, max_n=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\programmering\\prosjekter\\Bird-Is-The-Word\\src\\BWDatasets.py:44\u001b[39m, in \u001b[36mTrainDataSet.__init__\u001b[39m\u001b[34m(self, img_path, labels_path, bb_path, transform, max_n)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Retrieve images from folder and match them with labels\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.img_paths = df[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m e: os.path.join(img_path, e)).to_list()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_paths(\u001b[38;5;28mself\u001b[39m.img_paths)\n\u001b[32m     46\u001b[39m bb_path = bb_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m bb_paths = df[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m e: os.path.join(bb_path, e)).to_list()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\programmering\\prosjekter\\Bird-Is-The-Word\\src\\BWDatasets.py:56\u001b[39m, in \u001b[36mTrainDataSet.validate_paths\u001b[39m\u001b[34m(self, paths)\u001b[39m\n\u001b[32m     54\u001b[39m path = Path(i)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid file path. Path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Invalid file path. Path dataset\\datasets\\lyngoy\\images\\blub does not exist"
     ]
    }
   ],
   "source": [
    "# LyngÃ¸y\n",
    "label_path = \"dataset/datasets/lyngoy/ringcodes.csv\"\n",
    "image_path = \"dataset/datasets/lyngoy/images\"\n",
    "bb_path = \"dataset/datasets/lyngoy/labels\"\n",
    "\n",
    "# RF\n",
    "# label_path = \"dataset/datasets/rf/ringcodes.csv\"\n",
    "# image_path = \"dataset/datasets/rf/images\"\n",
    "# bb_path = \"dataset/datasets/rf/labels\"\n",
    "\n",
    "# ringmerkinno\n",
    "# label_path = \"dataset/datasets/ringmerkingno/ringcodes.csv\"\n",
    "# image_path = \"dataset/datasets/ringmerkingno/images\"\n",
    "# bb_path = \"dataset/datasets/ringmerkingno/labels\"\n",
    "\n",
    "\n",
    "\n",
    "max_n = 100\n",
    "transform = T.Compose([\n",
    "    # T.Resize((64,32)),\n",
    "    T.Resize((224,112)),\n",
    "    T.Lambda(lambda x: F.rotate(x, 270, expand=True))\n",
    "])\n",
    "exp_dataset = BWDatasets.TrainDataSet(img_path=image_path, labels_path=label_path, bb_path=bb_path, transform=transform, max_n=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c628614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(img):\n",
    "    img_uint8 = (img * 255).to(dtype=torch.uint8)\n",
    "    img_uint8 = img_uint8.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(img_uint8)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b40e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 of 3599\n",
      "Image 10 of 3599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(exp_dataset)):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     data = exp_dataset[i]\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i%\u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(exp_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\programmering\\prosjekter\\Bird-Is-The-Word\\src\\BWDatasets.py:140\u001b[39m, in \u001b[36mTrainDataSet.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# ocr_image = cv2.cvtColor(ocr_image, cv2.COLOR_RGB2GRAY)\u001b[39;00m\n\u001b[32m    108\u001b[39m \n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Normalization with grayscale, imagenet values\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Make sure that the image is a tensor\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(image, torch.Tensor)):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     image = T.ToTensor()(image)\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Make sure that the ocr image is a tensor\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(ocr_image, torch.Tensor)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.to_tensor(pic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m    152\u001b[39m     pic = pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m img = torch.from_numpy(pic.transpose((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))).contiguous()\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data = exp_dataset[1]\n",
    "\n",
    "\n",
    "draw_image(data[\"ocr_image\"])\n",
    "draw_image(data[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa415b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\legom\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|ââââââââââ| 6/6 [00:00<00:00, 4256.74it/s]\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\legom\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|ââââââââââ| 6/6 [00:00<00:00, 3845.63it/s]\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\legom\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|ââââââââââ| 6/6 [00:00<00:00, 4284.27it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in C:\\Users\\legom\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|ââââââââââ| 6/6 [00:00<00:00, 2863.33it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in C:\\Users\\legom\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|ââââââââââ| 6/6 [00:00<00:00, 1966.39it/s]\n"
     ]
    }
   ],
   "source": [
    "paddle.device.set_device(\"gpu:0\")\n",
    "ocr = PaddleOCR(lang=\"en\", \n",
    "        #    text_detection_model_dir=\"src/models/paddleOCR/det\", \n",
    "        #    text_recognition_model_dir=\"src/models/paddleOCR/rec\", \n",
    "        #    textline_orientation_model_dir=\"src/models/paddleOCR/cls\", \n",
    "           use_doc_orientation_classify=True, \n",
    "           use_doc_unwarping=True, \n",
    "           use_textline_orientation=True, device='gpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e783886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Run a test inference and watch your GPU\n",
    "_ = ocr.predict(\"other/sign1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab310a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_loader = DataLoader(dataset=exp_dataset, batch_size=64, shuffle=True, generator=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(dataloader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    count = 0\n",
    "    for data in dataloader:\n",
    "        count +=1\n",
    "        print(f'Batch {count} of {len(dataloader)}')\n",
    "        images = BWDatasets.tensor_to_numpy(data[\"ocr_image\"])\n",
    "        labels = data[\"label\"][0]\n",
    "        #print(images.shape)\n",
    "        for i, image in enumerate(images):\n",
    "\n",
    "            #print(image.shape)\n",
    "\n",
    "            result = ocr.predict(image)\n",
    "\n",
    "            pred = result[0][\"rec_texts\"]\n",
    "\n",
    "            if not pred:\n",
    "                pred = ['Not Found']\n",
    "\n",
    "            preds.append(pred[0])\n",
    "\n",
    "            label = labels[i]\n",
    "            \n",
    "            targets.append(label)\n",
    "\n",
    "            # print(\"-------------------\")\n",
    "            # print(f'Prediction: {pred}')\n",
    "            # print(f'Actual: {label}')\n",
    "\n",
    "            # plt.figure(figsize=(4,2))\n",
    "            # plt.imshow(image, cmap=\"gray\")\n",
    "            # plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "    return preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(preds, labels):\n",
    "    print(f'preds: {preds}')\n",
    "    print(f'labels: {labels}')\n",
    "    \n",
    "    # Boolean mask for the predictions and the labels\n",
    "    # Prediction = 1 if it is correct, 0 otherwise\n",
    "    # Labels are always = to 1\n",
    "    bin_preds = [int(pred == label) for pred, label in zip(preds, labels)]\n",
    "    bin_labels = [1]*len(labels)\n",
    "\n",
    "    preds_tensor, labels_tensor = torch.tensor(bin_preds), torch.tensor(bin_labels)\n",
    "    fbeta = FBetaScore(task='binary', beta=0.5)\n",
    "    acc = Accuracy(task='binary')\n",
    "\n",
    "    print(f'Model F1 Score: {fbeta(preds_tensor, labels_tensor)}')\n",
    "    print(f'Model Accuracy: {acc(preds_tensor, labels_tensor)}')\n",
    "    # print(f'preds: {preds_tensor}')\n",
    "    # print(f'labesl: {labels_tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 of 57\n",
      "Batch 2 of 57\n",
      "Batch 3 of 57\n",
      "Batch 4 of 57\n",
      "Batch 5 of 57\n",
      "Batch 6 of 57\n",
      "Batch 7 of 57\n",
      "Batch 8 of 57\n",
      "Batch 9 of 57\n",
      "Batch 10 of 57\n",
      "Batch 11 of 57\n",
      "Batch 12 of 57\n",
      "Batch 13 of 57\n",
      "Batch 14 of 57\n",
      "Batch 15 of 57\n",
      "Batch 16 of 57\n",
      "Batch 17 of 57\n",
      "Batch 18 of 57\n",
      "Batch 19 of 57\n",
      "Batch 20 of 57\n",
      "Batch 21 of 57\n",
      "Batch 22 of 57\n",
      "Batch 23 of 57\n",
      "Batch 24 of 57\n",
      "Batch 25 of 57\n",
      "Batch 26 of 57\n",
      "Batch 27 of 57\n",
      "Batch 28 of 57\n",
      "Batch 29 of 57\n",
      "Batch 30 of 57\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/datasets/lyngoy/images\\\\20190328_gramake_4k+_JL414_lyngoy_ost6_.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m preds, labels = prediction(exp_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mprediction\u001b[39m\u001b[34m(dataloader)\u001b[39m\n\u001b[32m      3\u001b[39m targets = []\n\u001b[32m      4\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m      6\u001b[39m     count +=\u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\programmering\\prosjekter\\Bird-Is-The-Word\\src\\BWDatasets.py:58\u001b[39m, in \u001b[36mTrainDataSet.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     55\u001b[39m bb_path = \u001b[38;5;28mself\u001b[39m.bb_paths[idx]\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Retrieve image into a numpy array\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m image = Image.open(\u001b[38;5;28mself\u001b[39m.img_paths[idx]).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     59\u001b[39m image = np.array(image)\n\u001b[32m     60\u001b[39m image = image[:, :, ::-\u001b[32m1\u001b[39m].copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\legom\\anaconda3\\envs\\birdword\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset/datasets/lyngoy/images\\\\20190328_gramake_4k+_JL414_lyngoy_ost6_.JPG'"
     ]
    }
   ],
   "source": [
    "preds, labels = prediction(exp_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(preds=preds, labels=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdword",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
